{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf069ef",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "md"
   },
   "outputs": [],
   "source": [
    "# # Install Dependencies and Fetch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dbcea6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "random_state = 1\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf85966",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "wine_quality = fetch_ucirepo(id=186)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = wine_quality.data.features\n",
    "y = wine_quality.data.targets\n",
    "\n",
    "# metadata\n",
    "print(wine_quality.metadata)\n",
    "\n",
    "# variable information\n",
    "print(wine_quality.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaa807d",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "md"
   },
   "outputs": [],
   "source": [
    "# ## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e49689e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# test and train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77849335",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "md"
   },
   "outputs": [],
   "source": [
    "# # Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a96076",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "md"
   },
   "outputs": [],
   "source": [
    "# ## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b468a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Display the first few rows of the features and target\n",
    "print(X_train.head())\n",
    "print(y_train.head())\n",
    "print(X_test.head())\n",
    "print(y_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d4b995",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Display the shape of the features and target\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c557f8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Display the data types of the features and target\n",
    "print(\"Data types of features (X):\")\n",
    "print(X_train.dtypes)\n",
    "print(\"\\nData types of target (y):\")\n",
    "print(y_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a2f068",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "md"
   },
   "outputs": [],
   "source": [
    "# ## Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e762fc3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "missing_values_X = X.isnull().sum()\n",
    "missing_values_y = y.isnull().sum()\n",
    "print(\"Missing values in features (X):\")\n",
    "print(missing_values_X)\n",
    "print(\"\\nMissing values in target (y):\")\n",
    "print(missing_values_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57201c70",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "md"
   },
   "outputs": [],
   "source": [
    "# ## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8f5621",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "summary_X_train = X_train.describe()\n",
    "print(\"Summary statistics for features (X_train):\")\n",
    "print(summary_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0692a536",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "summary_y_train = y_train.describe()\n",
    "print(\"\\nSummary statistics for target (y_train):\")\n",
    "print(summary_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab0b967",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "md"
   },
   "outputs": [],
   "source": [
    "# ## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfc1472",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a986a0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Histograms for each feature\n",
    "X_train.hist(figsize=(12, 10))\n",
    "plt.suptitle(\"Histograms of Features\", y=1.02, fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590f5e5a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Box plots for each feature\n",
    "plt.figure(figsize=(12, 10))\n",
    "for i, column in enumerate(X_train.columns, 1):\n",
    "    plt.subplot(3, 4, i)\n",
    "    sns.boxplot(x=X_train[column])\n",
    "    plt.title(f\"Box Plot of {column}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4fa3b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Histogram of quality\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(y_train[\"quality\"], kde=True)\n",
    "plt.title(\"Histogram of quality\", fontsize=16)\n",
    "plt.xlabel(\"quality\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf4682",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Box Plot of quality\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=y_train[\"quality\"])\n",
    "plt.title(\"Box Plot of quality\", fontsize=16)\n",
    "plt.xlabel(\"quality\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68803f82",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "md"
   },
   "outputs": [],
   "source": [
    "# ## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9955f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = X_train.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', square=True)\n",
    "plt.title('Correlation Heatmap of Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b146ac9",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "md"
   },
   "outputs": [],
   "source": [
    "# # Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5f2736",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "md"
   },
   "outputs": [],
   "source": [
    "# ## Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c85387",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "md"
   },
   "outputs": [],
   "source": [
    "# The goal is to optimize the wine quality using Bayesian optimization. The objective function will be defined to minimize the negative of the quality, as we want to maximize it. The parameters to be optimized will include the features of the wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705e41b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b4b867",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = RandomForestRegressor(random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a78356",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Define the search space for hyperparameters\n",
    "search_space = {\n",
    "    'n_estimators': (50, 500),  # Number of trees in the forest\n",
    "    'max_depth': (5, 50),        # Maximum depth of the tree\n",
    "    'min_samples_split': (2, 20), # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': (1, 20),  # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': ['sqrt', 'log2', None],  # Number of features to consider when looking for the best split\n",
    "    'bootstrap': [True, False]  # Whether bootstrap samples are used when building trees\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe6c053",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Define the Bayesian optimization search\n",
    "opt = BayesSearchCV(\n",
    "    model,\n",
    "    search_space,\n",
    "    n_iter=50,  # Number of iterations for optimization\n",
    "    scoring='neg_mean_squared_error',  # Objective function to minimize\n",
    "    cv=5,  # Cross-validation splitting strategy\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea4f379",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Fit the model using Bayesian optimization\n",
    "opt.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9315b4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Display the best parameters found by Bayesian optimization\n",
    "print(\"Best parameters found by Bayesian optimization:\")\n",
    "pprint(opt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11db5d47",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Display the best score achieved\n",
    "print(f\"Best score achieved (negative MSE): {opt.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e164ad",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Evaluate the optimized model on the test set\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = opt.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error on test set: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863185e1",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "md"
   },
   "outputs": [],
   "source": [
    "# ## Use Optimized Model to Optimize Quality using Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc18c82",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Use the optimized model to predict quality\n",
    "optimized_quality = opt.predict(X_test)\n",
    "print(\"Predicted quality using Optimized Model:\")\n",
    "print(optimized_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b49f18",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Visualize the predicted vs actual quality\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, optimized_quality, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.title('Predicted vs Actual quality', fontsize=16)\n",
    "plt.xlabel('Actual quality')\n",
    "plt.ylabel('Predicted quality')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eec652",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# First check the actual column names in the DataFrame\n",
    "print(\"Actual column names in X_train:\")\n",
    "print(X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab7f0e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Objective function for Bayesian optimization\n",
    "def objective_function(params):\n",
    "    \"\"\"\n",
    "    Objective function to minimize the negative of the quality.\n",
    "    This function takes a the parameters of the wine and returns the negative quality.\n",
    "    \"\"\"\n",
    "    # Unpack the parameters\n",
    "    fixed_acidity, volatile_acidity, citric_acid, residual_sugar, chlorides, free_sulfur_dioxide, total_sulfur_dioxide, density, pH, sulphates, alcohol = params\n",
    "    # Create a DataFrame with the parameters\n",
    "    wine_feat = pd.DataFrame({\n",
    "        'fixed_acidity': [fixed_acidity],\n",
    "        'volatile_acidity': [volatile_acidity],\n",
    "        'citric_acid': [citric_acid],\n",
    "        'residual_sugar': [residual_sugar],\n",
    "        'chlorides': [chlorides],\n",
    "        'free_sulfur_dioxide': [free_sulfur_dioxide],\n",
    "        'total_sulfur_dioxide': [total_sulfur_dioxide],\n",
    "        'density': [density],\n",
    "        'pH': [pH],\n",
    "        'sulphates': [sulphates],\n",
    "        'alcohol': [alcohol]\n",
    "    })\n",
    "    # Predict the quality using the optimized model\n",
    "    predicted_quality = opt.predict(wine_feat)\n",
    "    # Return the negative quality (as we want to maximize it)\n",
    "    return -predicted_quality[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ae588a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Define the search space for the parameters of the wine based on the min and max values in the training set\n",
    "search_space_wine = [\n",
    "    (X_train['fixed_acidity'].min(), X_train['fixed_acidity'].max()),  # fixed_acidity\n",
    "    (X_train['volatile_acidity'].min(), X_train['volatile_acidity'].max()),  # Slag\n",
    "    (X_train['citric_acid'].min(), X_train['citric_acid'].max()),  # Ash\n",
    "    (X_train['residual_sugar'].min(), X_train['residual_sugar'].max()),  # residual_sugar\n",
    "    (X_train['chlorides'].min(), X_train['chlorides'].max()),  # chlorides\n",
    "    (X_train['free_sulfur_dioxide'].min(), X_train['free_sulfur_dioxide'].max()),  # free_sulfur_dioxide\n",
    "    (X_train['total_sulfur_dioxide'].min(), X_train['total_sulfur_dioxide'].max()),  # total_sulfur_dioxide\n",
    "    (X_train['density'].min(), X_train['density'].max()),  # density\n",
    "    (X_train['pH'].min(), X_train['pH'].max()),  # pH\n",
    "    (X_train['sulphates'].min(), X_train['sulphates'].max()),  # sulphates\n",
    "    (X_train['alcohol'].min(), X_train['alcohol'].max())  # alcohol\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e691ff",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "# Perform Bayesian optimization to find the optimal wine parameters\n",
    "result = gp_minimize(\n",
    "    objective_function,\n",
    "    search_space_wine,\n",
    "    n_calls=50,  # Number of evaluations\n",
    "    random_state=random_state,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb60774",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Neatly display the best parameters and the best predicted quality along with column names\n",
    "best_params = result.x\n",
    "best_quality = -result.fun  # Negate the result to get the actual quality\n",
    "print(\"Best parameters found by Bayesian optimization:\")\n",
    "for i, param in enumerate(best_params):\n",
    "    print(f\"{X_train.columns[i]}: {param}\")\n",
    "print(f\"Best predicted quality: {best_quality}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0441cb",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "md"
   },
   "outputs": [],
   "source": [
    "# ## Use XGBoost for the Model instead of Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798be063",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becde4d4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Define the XGBoost model\n",
    "xgb_model = XGBRegressor(random_state=random_state, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d725d28a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Define the Bayesian optimization search for XGBoost hyperparameters\n",
    "xgb_search_space = {\n",
    "    'n_estimators': (50, 500),  # Number of trees in the forest\n",
    "    'max_depth': (3, 10),        # Maximum depth of the tree\n",
    "    'learning_rate': (0.01, 0.3, 'uniform'),  # Step size shrinkage used in update to prevent overfitting\n",
    "    'subsample': (0.5, 1.0, 'uniform'),  # Subsample ratio of the training instances\n",
    "    'colsample_bytree': (0.5, 1.0, 'uniform'),  # Subsample ratio of columns when constructing each tree\n",
    "    'gamma': (0, 5),  # Minimum loss reduction required to make a further partition on a leaf\n",
    "    'reg_alpha': (0, 1),  # L1 regularization term on weights\n",
    "    'reg_lambda': (0, 1)  # L2 regularization term on weights\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f316b4f7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Define the Bayesian optimization search for XGBoost\n",
    "xgb_opt = BayesSearchCV(\n",
    "    xgb_model,\n",
    "    xgb_search_space,\n",
    "    n_iter=50,  # Number of iterations for optimization\n",
    "    scoring='neg_mean_squared_error',  # Objective function to minimize\n",
    "    cv=5,  # Cross-validation splitting strategy\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df2539b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Fit the XGBoost model using Bayesian optimization\n",
    "xgb_opt.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f141d8f0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Display the best parameters found by Bayesian optimization for XGBoost\n",
    "print(\"Best parameters found by Bayesian optimization for XGBoost:\")\n",
    "pprint(xgb_opt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c10b4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Display the best score achieved by XGBoost\n",
    "print(f\"Best score achieved (negative MSE) by XGBoost: {xgb_opt.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f86af7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Evaluate the optimized XGBoost model on the test set\n",
    "y_pred_xgb = xgb_opt.predict(X_test)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "print(f\"Mean Squared Error on test set by XGBoost: {mse_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73d866b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Use the optimized XGBoost model to predict quality\n",
    "optimized_quality_xgb = xgb_opt.predict(X_test)\n",
    "print(\"Predicted quality using Optimized XGBoost Model:\")\n",
    "print(optimized_quality_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01273187",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Objective function for Bayesian optimization with XGBoost\n",
    "def objective_function_xgb(params):\n",
    "    \"\"\"\n",
    "    Objective function to minimize the negative of the quality using XGBoost.\n",
    "    This function takes the parameters of the wine and returns the negative quality.\n",
    "    \"\"\"\n",
    "    # Unpack the parameters\n",
    "    fixed_acidity, volatile_acidity, citric_acid, residual_sugar, chlorides, free_sulfur_dioxide, total_sulfur_dioxide, density, pH, sulphates, alcohol = params\n",
    "    # Create a DataFrame with the parameters\n",
    "    wine_feat = pd.DataFrame({\n",
    "        'fixed_acidity': [fixed_acidity],\n",
    "        'volatile_acidity': [volatile_acidity],\n",
    "        'citric_acid': [citric_acid],\n",
    "        'residual_sugar': [residual_sugar],\n",
    "        'chlorides': [chlorides],\n",
    "        'free_sulfur_dioxide': [free_sulfur_dioxide],\n",
    "        'total_sulfur_dioxide': [total_sulfur_dioxide],\n",
    "        'density': [density],\n",
    "        'pH': [pH],\n",
    "        'sulphates': [sulphates],\n",
    "        'alcohol': [alcohol]\n",
    "    })\n",
    "    # Predict the quality using the optimized XGBoost model\n",
    "    predicted_quality = xgb_opt.predict(wine_feat)\n",
    "    # Return the negative quality (as we want to maximize it)\n",
    "    return -predicted_quality[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b746832",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Define the search space for the parameters of the wine based on the min and max values in the training set\n",
    "search_space_wine_xgb = [\n",
    "    (X_train['fixed_acidity'].min(), X_train['fixed_acidity'].max()),  # fixed_acidity\n",
    "    (X_train['volatile_acidity'].min(), X_train['volatile_acidity'].max()),  # Slag\n",
    "    (X_train['citric_acid'].min(), X_train['citric_acid'].max()),  # Ash\n",
    "    (X_train['residual_sugar'].min(), X_train['residual_sugar'].max()),  # residual_sugar\n",
    "    (X_train['chlorides'].min(), X_train['chlorides'].max()),  # chlorides\n",
    "    (X_train['free_sulfur_dioxide'].min(), X_train['free_sulfur_dioxide'].max()),  # free_sulfur_dioxide\n",
    "    (X_train['total_sulfur_dioxide'].min(), X_train['total_sulfur_dioxide'].max()),  # total_sulfur_dioxide\n",
    "    (X_train['density'].min(), X_train['density'].max()),  # density\n",
    "    (X_train['pH'].min(), X_train['pH'].max()),  # pH\n",
    "    (X_train['sulphates'].min(), X_train['sulphates'].max()),  # sulphates\n",
    "    (X_train['alcohol'].min(), X_train['alcohol'].max())  # alcohol\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cf3851",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "# Perform Bayesian optimization to find the optimal wine parameters using XGBoost\n",
    "result_xgb = gp_minimize(\n",
    "    objective_function_xgb,\n",
    "    search_space_wine_xgb,\n",
    "    n_calls=50,  # Number of evaluations\n",
    "    random_state=random_state,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a42cf8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Neatly display the best parameters and the best predicted quality along with column names for XGBoost\n",
    "best_params_xgb = result_xgb.x\n",
    "best_quality_xgb = -result_xgb.fun  # Negate the result to get the actual quality\n",
    "print(\"Best parameters found by Bayesian optimization for XGBoost:\")\n",
    "for i, param in enumerate(best_params_xgb):\n",
    "    print(f\"{X_train.columns[i]}: {param}\")\n",
    "print(f\"Best predicted quality using XGBoost: {best_quality_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d395c9",
   "metadata": {
    "title": "md"
   },
   "outputs": [],
   "source": [
    "# # Human-in-the-Loop Preference Learning for Bayesian Optimization\n",
    "#\n",
    "# In this section, we will implement a Human-in-the-Loop (HITL) approach to guide the Bayesian optimization process using preference learning. We will simulate a human expert to provide subjective feedback on wine profiles, which will be used to train a user belief model. This model will then be combined with the main surrogate model to create a more informed acquisition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579cb6d",
   "metadata": {
    "title": "md"
   },
   "outputs": [],
   "source": [
    "# ## Step 1: Simulate the Human Expert\n",
    "#\n",
    "# We begin by creating a function that simulates a human expert's preferences. This function will compare two wine profiles and indicate a preference based on their proximity to a \"golden standard\" profile, which we define using the best parameters found by the XGBoost optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b79db3a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Golden standard profile based on XGBoost optimization results\n",
    "golden_standard_profile = result_xgb.x\n",
    "\n",
    "def simulate_human_expert(profile1, profile2):\n",
    "    \"\"\"\n",
    "    Simulates a human expert's preference between two wine profiles.\n",
    "    The preference is based on the Euclidean distance to a golden standard profile.\n",
    "\n",
    "    Args:\n",
    "        profile1 (list): The first wine profile.\n",
    "        profile2 (list): The second wine profile.\n",
    "\n",
    "    Returns:\n",
    "        int: 1 if profile1 is preferred, 0 otherwise.\n",
    "    \"\"\"\n",
    "    dist1 = np.linalg.norm(np.array(profile1) - golden_standard_profile)\n",
    "    dist2 = np.linalg.norm(np.array(profile2) - golden_standard_profile)\n",
    "    if dist1 < dist2:\n",
    "        return 1  # Prefers profile1\n",
    "    else:\n",
    "        return 0  # Prefers profile2\n",
    "\n",
    "# Example usage:\n",
    "# Create two random profiles for demonstration\n",
    "random_profile1 = [X_train.iloc[0, i] for i in range(X_train.shape[1])]\n",
    "random_profile2 = [X_train.iloc[1, i] for i in range(X_train.shape[1])]\n",
    "preference = simulate_human_expert(random_profile1, random_profile2)\n",
    "print(f\"Simulated expert preference: {preference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27494ab2",
   "metadata": {
    "title": "md"
   },
   "outputs": [],
   "source": [
    "# ## Step 2: Implement the Preference Learning Component\n",
    "#\n",
    "# Next, we implement the preference learning component. This involves generating pairs of candidate wine profiles, eliciting preferences from our simulated expert, and training a user belief model—a Gaussian Process Classifier (GPC)—on this preference data. The GPC will learn to predict the expert's preferences, which will help guide the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c84beb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings from GPC\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# Initialize the user belief model (GPC)\n",
    "# A radial-basis function (RBF) kernel is a common choice\n",
    "kernel = 1.0 * RBF(length_scale=1.0)\n",
    "user_belief_model = GaussianProcessClassifier(kernel=kernel, random_state=random_state)\n",
    "\n",
    "# Generate initial preference data to train the GPC\n",
    "n_initial_pairs = 10\n",
    "preference_data = []\n",
    "preference_labels = []\n",
    "\n",
    "for _ in range(n_initial_pairs):\n",
    "    # Generate two random profiles from the search space\n",
    "    x1 = [np.random.uniform(low, high) for low, high in search_space_wine_xgb]\n",
    "    x2 = [np.random.uniform(low, high) for low, high in search_space_wine_xgb]\n",
    "\n",
    "    # Get the simulated expert's preference\n",
    "    preference = simulate_human_expert(x1, x2)\n",
    "\n",
    "    # Store the preference data\n",
    "    # We create a feature vector that is the difference between the two profiles\n",
    "    preference_data.append(np.array(x1) - np.array(x2))\n",
    "    preference_labels.append(preference)\n",
    "\n",
    "# Train the initial user belief model\n",
    "user_belief_model.fit(preference_data, preference_labels)\n",
    "\n",
    "print(\"Initial user belief model trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f08e0f",
   "metadata": {
    "title": "md"
   },
   "outputs": [],
   "source": [
    "# ## Step 3: Modify the Bayesian Optimization Loop\n",
    "#\n",
    "# Now, we integrate the user belief model into the Bayesian optimization loop. We'll create a new acquisition function that combines the predictions from our main surrogate model (XGBoost) and the user belief model (GPC). This new function will guide the selection of candidate profiles by balancing predicted quality with the simulated expert's preferences. A custom optimization loop is implemented to accommodate this HITL approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af544f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from skopt.utils import cook_initial_point_generator\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "n_iterations = 50\n",
    "n_candidates_per_iteration = 100  # Number of random candidates to evaluate with the acquisition function\n",
    "acquisition_weight = 0.5  # Weight for combining surrogate and belief models\n",
    "\n",
    "# --- Initialization ---\n",
    "# Use the best XGBoost model as the main surrogate\n",
    "main_surrogate_model = xgb_opt.best_estimator_\n",
    "\n",
    "# Store the history of evaluated points and their objective values\n",
    "evaluated_points = []\n",
    "objective_values = []\n",
    "\n",
    "# Store the convergence history\n",
    "convergence_hitl = []\n",
    "best_quality_so_far = -np.inf\n",
    "\n",
    "# --- Custom Optimization Loop ---\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    print(f\"--- Iteration {i+1}/{n_iterations} ---\")\n",
    "\n",
    "    # 1. Generate candidate profiles to evaluate with the acquisition function\n",
    "    # We'll use a simple random sampling from the search space\n",
    "    candidate_generator = cook_initial_point_generator(\"random\")\n",
    "    candidates = candidate_generator.generate(search_space_wine_xgb, n_candidates_per_iteration)\n",
    "\n",
    "    # 2. Define and evaluate the new acquisition function for each candidate\n",
    "    def acquisition_function(x):\n",
    "        x_df = pd.DataFrame([x], columns=X_train.columns)\n",
    "\n",
    "        # a) Prediction from the main surrogate model (XGBoost)\n",
    "        # We want to maximize quality, so we take the direct prediction\n",
    "        pred_quality = main_surrogate_model.predict(x_df)[0]\n",
    "\n",
    "        # b) Prediction from the user belief model (GPC)\n",
    "        # The GPC gives the probability of a profile being \"good\" (preferred)\n",
    "        # We use predict_proba to get the probability of class 1 (preferred)\n",
    "        pred_preference = user_belief_model.predict_proba(np.array(x).reshape(1, -1))[0][1]\n",
    "\n",
    "        # c) Combine the two predictions (weighted average)\n",
    "        # We want to maximize this combined score\n",
    "        return acquisition_weight * pred_quality + (1 - acquisition_weight) * pred_preference\n",
    "\n",
    "    # Evaluate acquisition function for all candidates\n",
    "    acquisition_scores = [acquisition_function(c) for c in candidates]\n",
    "\n",
    "    # 3. Select the best candidate (the one that maximizes the acquisition function)\n",
    "    best_candidate_index = np.argmax(acquisition_scores)\n",
    "    next_point = candidates[best_candidate_index]\n",
    "    print(f\"Selected new point to evaluate.\")\n",
    "\n",
    "    # 4. Evaluate the selected point with the true objective function\n",
    "    # In a real scenario, this would be a real experiment. Here we use our objective_function_xgb.\n",
    "    true_objective_value = -objective_function_xgb(next_point) # Negate to get quality\n",
    "    print(f\"True quality of new point: {true_objective_value:.4f}\")\n",
    "\n",
    "    # Store the results\n",
    "    evaluated_points.append(next_point)\n",
    "    objective_values.append(true_objective_value)\n",
    "\n",
    "    # Update convergence tracking\n",
    "    if true_objective_value > best_quality_so_far:\n",
    "        best_quality_so_far = true_objective_value\n",
    "    convergence_hitl.append(best_quality_so_far)\n",
    "\n",
    "    # 5. Update the user belief model with new preference data\n",
    "    # To do this, we need a pair. We'll pair the new point with a random previous point.\n",
    "    if len(evaluated_points) > 1:\n",
    "        # Select a random point from the history to form a pair\n",
    "        random_index = np.random.randint(0, len(evaluated_points) - 1)\n",
    "        profile1 = next_point\n",
    "        profile2 = evaluated_points[random_index]\n",
    "\n",
    "        # Get simulated expert preference\n",
    "        preference = simulate_human_expert(profile1, profile2)\n",
    "\n",
    "        # Update the GPC with the new preference data\n",
    "        # The GPC expects the difference between profiles\n",
    "        if preference == 1: # Prefers profile1\n",
    "            new_data_point = np.array(profile1) - np.array(profile2)\n",
    "            new_label = 1\n",
    "        else: # Prefers profile2\n",
    "            new_data_point = np.array(profile2) - np.array(profile1)\n",
    "            new_label = 1 # The label is always 1 for the preferred profile's difference vector\n",
    "\n",
    "        # Append new data and retrain the model\n",
    "        preference_data.append(new_data_point)\n",
    "        preference_labels.append(new_label)\n",
    "        user_belief_model.fit(preference_data, preference_labels)\n",
    "        print(\"User belief model updated.\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nHITL optimization finished in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- Final Results ---\n",
    "best_hitl_index = np.argmax(objective_values)\n",
    "best_params_hitl = evaluated_points[best_hitl_index]\n",
    "best_quality_hitl = objective_values[best_hitl_index]\n",
    "\n",
    "print(\"\\nBest parameters found by HITL Bayesian optimization:\")\n",
    "for i, param in enumerate(best_params_hitl):\n",
    "    print(f\"{X_train.columns[i]}: {param}\")\n",
    "print(f\"\\nBest predicted quality using HITL: {best_quality_hitl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887a9d2f",
   "metadata": {
    "title": "md"
   },
   "outputs": [],
   "source": [
    "# ## Step 4: Final Evaluation\n",
    "#\n",
    "# Finally, we evaluate the performance of our Human-in-the-Loop (HITL) Bayesian optimization and compare it to the original XGBoost-based optimization. We will plot the convergence of both methods to see which one finds a better solution faster and compare the best wine profiles discovered by each approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db32762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare Data for Comparison ---\n",
    "# Get the convergence data from the original gp_minimize result\n",
    "# The objective function for gp_minimize returns the negative of quality, so we negate it back\n",
    "convergence_original = np.maximum.accumulate(-np.array(result_xgb.func_vals))\n",
    "\n",
    "# Ensure both convergence plots are of the same length for a fair comparison\n",
    "# We'll cap it at the number of iterations we ran for the HITL loop\n",
    "num_evaluations = min(len(convergence_hitl), len(convergence_original))\n",
    "convergence_hitl_plot = convergence_hitl[:num_evaluations]\n",
    "convergence_original_plot = convergence_original[:num_evaluations]\n",
    "\n",
    "# --- Plotting the Convergence ---\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(range(1, num_evaluations + 1), convergence_hitl_plot, 'o-', label='HITL Bayesian Optimization', color='blue')\n",
    "plt.plot(range(1, num_evaluations + 1), convergence_original_plot, 's-', label='Original Bayesian Optimization (XGBoost)', color='green')\n",
    "plt.title('Convergence Comparison: HITL vs. Original Bayesian Optimization', fontsize=16)\n",
    "plt.xlabel('Number of Evaluations', fontsize=12)\n",
    "plt.ylabel('Best Quality Found So Far', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()\n",
    "\n",
    "# --- Comparing the Best Results ---\n",
    "print(\"--- Comparison of Best Results ---\")\n",
    "print(\"\\nOriginal Bayesian Optimization (XGBoost):\")\n",
    "print(f\"  Best Quality: {best_quality_xgb:.4f}\")\n",
    "print(\"  Best Profile:\")\n",
    "for i, param in enumerate(best_params_xgb):\n",
    "    print(f\"    {X_train.columns[i]}: {param:.4f}\")\n",
    "\n",
    "print(\"\\nHITL Bayesian Optimization:\")\n",
    "print(f\"  Best Quality: {best_quality_hitl:.4f}\")\n",
    "print(\"  Best Profile:\")\n",
    "for i, param in enumerate(best_params_hitl):\n",
    "    print(f\"    {X_train.columns[i]}: {param:.4f}\")\n",
    "\n",
    "# --- Analysis ---\n",
    "print(\"\\n--- Analysis ---\")\n",
    "if best_quality_hitl > best_quality_xgb:\n",
    "    print(\"The HITL approach found a wine profile with a higher predicted quality.\")\n",
    "    print(f\"Quality improvement: {best_quality_hitl - best_quality_xgb:.4f}\")\n",
    "else:\n",
    "    print(\"The original Bayesian optimization found a wine profile with a slightly better or equal quality.\")\n",
    "\n",
    "print(\"\\nBy observing the convergence plot, we can analyze which method converged faster to a high-quality solution.\")\n",
    "print(\"The HITL approach, by incorporating user preferences, can sometimes explore more promising regions of the search space, potentially leading to faster discovery of high-quality profiles.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
